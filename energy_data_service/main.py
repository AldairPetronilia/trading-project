"""
Main application entry point for the Energy Data Service.

This module provides a production-ready entry point for running the energy data
collection service. Key features include:

- Configurable logging based on environment settings
- Database initialization with TimescaleDB hypertables
- ENTSO-E API configuration validation
- Startup backfill analysis and gap filling
- Data collection verification and summary statistics
- Graceful shutdown handling with signal management
- Comprehensive error handling with user-friendly messages

The service automatically:
1. Validates database connectivity and schema
2. Configures TimescaleDB for time-series data
3. Validates ENTSO-E API credentials
4. Starts the scheduler service for automated data collection
5. Performs startup backfill to fill historical gaps (configurable)
6. Shows data collection summary and statistics
7. Runs continuous data collection until shutdown signal
"""

import asyncio
import contextlib
import logging
import signal
import sys
from datetime import UTC, datetime, timedelta
from typing import Any

import uvicorn
from app.api.app import create_app
from app.container import Container
from app.exceptions.service_exceptions import BackfillError, ServiceError
from app.models.base import Base
from app.models.load_data import EnergyDataPoint
from sqlalchemy import func, select, text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncConnection

# Constants
MIN_API_TOKEN_LENGTH = 10


class SimpleSchedulerRunner:
    """
    Production application runner for the Energy Data Service.

    Orchestrates the complete lifecycle of the energy data collection service,
    including database setup, configuration validation, scheduler startup,
    backfill operations, and graceful shutdown handling.
    """

    def __init__(self) -> None:
        """Initialize the runner with container and shutdown handling."""
        self.container = Container()
        self.shutdown_event = asyncio.Event()

        # Get settings for logging configuration
        settings = self.container.config()

        # Setup logging based on configuration
        log_level = getattr(logging, settings.logging.level, logging.INFO)
        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

        self.logger = logging.getLogger(__name__)

    def setup_signal_handlers(self) -> None:
        """Setup graceful shutdown signal handlers."""

        def signal_handler(signum: int, _frame: Any) -> None:
            self.logger.info("Received shutdown signal: %s", signum)
            self.shutdown_event.set()

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

    async def _verify_data_collection(self) -> None:
        """Verify data collection by showing summary statistics."""
        try:
            self.logger.info("Verifying data collection...")

            # Get database from container
            database = self.container.database()

            async with database.session_factory() as session:
                # Count total data points
                total_count_stmt = select(func.count(EnergyDataPoint.timestamp))
                total_result = await session.execute(total_count_stmt)
                total_count = total_result.scalar() or 0

                if total_count == 0:
                    self.logger.warning("No data points found in database")
                    return

                # Get date range
                min_date_stmt = select(func.min(EnergyDataPoint.timestamp))
                max_date_stmt = select(func.max(EnergyDataPoint.timestamp))

                min_result = await session.execute(min_date_stmt)
                max_result = await session.execute(max_date_stmt)

                min_date = min_result.scalar()
                max_date = max_result.scalar()

                # Count unique areas
                unique_areas_stmt = select(
                    func.count(func.distinct(EnergyDataPoint.area_code))
                )
                areas_result = await session.execute(unique_areas_stmt)
                unique_areas = areas_result.scalar() or 0

                # Count recent data (last 24 hours)
                recent_cutoff = datetime.now(UTC) - timedelta(days=1)
                recent_count_stmt = select(func.count(EnergyDataPoint.timestamp)).where(
                    EnergyDataPoint.timestamp >= recent_cutoff
                )
                recent_result = await session.execute(recent_count_stmt)
                recent_count = recent_result.scalar() or 0

                # Log summary
                self.logger.info(
                    "Data Collection Summary:\n"
                    "  Total data points: %s\n"
                    "  Date range: %s to %s\n"
                    "  Unique areas: %s\n"
                    "  Recent data (24h): %s points",
                    f"{total_count:,}",
                    min_date,
                    max_date,
                    unique_areas,
                    f"{recent_count:,}",
                )

        except Exception:
            self.logger.exception("Data verification failed")
            # Don't fail startup if verification fails

    async def _perform_startup_backfill(self) -> None:
        """Perform backfill analysis and fill gaps on startup using configured settings."""
        try:
            self.logger.info("Starting startup backfill analysis...")

            # Get backfill service and settings from container
            backfill_service = self.container.backfill_service()
            settings = self.container.config()

            # Calculate coverage analysis period based on backfill config
            analysis_years = settings.backfill.historical_years
            self.logger.info("Analyzing coverage for last %s years", analysis_years)

            coverage_results = await backfill_service.analyze_coverage(
                years_back=analysis_years
            )

            # Find areas that need backfilling
            needed_backfills = [r for r in coverage_results if r.needs_backfill]

            self.logger.info(
                "Coverage analysis complete: %s of %s "
                "area/endpoint combinations need backfilling",
                len(needed_backfills),
                len(coverage_results),
            )

            # Track statistics for summary
            total_areas_processed = 0
            total_data_points = 0
            successful_backfills = 0
            failed_backfills = 0

            if needed_backfills:
                self.logger.info(
                    "Processing backfills for all %s area/endpoint combinations",
                    len(needed_backfills),
                )

                for result in needed_backfills:
                    total_areas_processed += 1
                    self.logger.info(
                        "Backfilling %s - %s (coverage: %.1f%%)",
                        result.area_code,
                        result.endpoint_name,
                        result.coverage_percentage,
                    )

                    try:
                        # Calculate backfill period using configured historical years
                        end_time = datetime.now(UTC)
                        start_time = end_time - timedelta(days=365 * analysis_years)

                        backfill_result = await backfill_service.start_backfill(
                            area_code=result.area_code,
                            endpoint_name=result.endpoint_name,
                            period_start=start_time,
                            period_end=end_time,
                        )

                        if backfill_result.success:
                            successful_backfills += 1
                            total_data_points += backfill_result.data_points_collected
                            self.logger.info(
                                "✓ Backfill completed for %s - %s: "
                                "%s data points collected",
                                result.area_code,
                                result.endpoint_name,
                                backfill_result.data_points_collected,
                            )
                        else:
                            failed_backfills += 1
                            error_msg = (
                                "; ".join(backfill_result.error_messages)
                                if backfill_result.error_messages
                                else "Unknown error"
                            )
                            self.logger.warning(
                                "✗ Backfill failed for %s - %s: %s",
                                result.area_code,
                                result.endpoint_name,
                                error_msg,
                            )

                    except (BackfillError, ServiceError, SQLAlchemyError) as e:
                        failed_backfills += 1
                        self.logger.warning(
                            "✗ Backfill failed for %s - %s: %s",
                            result.area_code,
                            result.endpoint_name,
                            e,
                        )
                        # Continue with other backfills even if one fails

                # Log summary statistics
                self.logger.info(
                    "Startup backfill summary: %s successful, %s failed, "
                    "%s total data points collected",
                    successful_backfills,
                    failed_backfills,
                    total_data_points,
                )
            else:
                self.logger.info("No backfills needed - data coverage is sufficient")

        except Exception:
            self.logger.exception("Startup backfill analysis failed")
            # Don't fail the entire startup if backfill fails
            self.logger.info("Continuing startup despite backfill failure")

    async def _initialize_database(self) -> None:
        """Initialize database connection and create tables."""
        try:
            database = self.container.database()
            self.logger.info("Database connection initialized")
        except Exception:
            self.logger.exception("Failed to initialize database connection")
            self.logger.exception(
                "Please check your database configuration and ensure the database is running"
            )
            sys.exit(1)

        # Create database tables if they don't exist
        try:
            async with database.engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
                await self._setup_timescaledb(conn)
            self.logger.info("Database schema initialized")
        except Exception:
            self.logger.exception("Failed to initialize database schema")
            self.logger.exception(
                "Please ensure TimescaleDB is properly installed and accessible"
            )
            sys.exit(1)

    async def _setup_timescaledb(self, conn: AsyncConnection) -> None:
        """Setup TimescaleDB extensions, hypertables and compression."""
        # Create TimescaleDB extension if not exists
        await conn.execute(text("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;"))

        # Create hypertables for time-series data
        try:
            await conn.execute(
                text(
                    "SELECT create_hypertable('energy_data_points', 'timestamp', "
                    "if_not_exists => TRUE);"
                )
            )
            self.logger.info("TimescaleDB hypertable for energy_data_points configured")
        except SQLAlchemyError:
            # Hypertable might already exist or table might not exist yet
            self.logger.debug("Energy data hypertable creation skipped")

        # Create hypertable for price data
        try:
            await conn.execute(
                text(
                    "SELECT create_hypertable('energy_price_points', 'timestamp', "
                    "if_not_exists => TRUE);"
                )
            )
            self.logger.info(
                "TimescaleDB hypertable for energy_price_points configured"
            )
        except SQLAlchemyError:
            # Hypertable might already exist or table might not exist yet
            self.logger.debug("Price data hypertable creation skipped")

        # Enable compression for better storage efficiency on energy data
        try:
            await conn.execute(
                text(
                    "ALTER TABLE energy_data_points SET ("
                    "timescaledb.compress, "
                    "timescaledb.compress_segmentby = 'area_code, data_type, business_type', "
                    "timescaledb.compress_orderby = 'timestamp DESC'"
                    ");"
                )
            )
            self.logger.info("TimescaleDB compression enabled for energy_data_points")
        except SQLAlchemyError:
            # Compression might already be enabled
            self.logger.debug("Energy data compression setup skipped")

        # Enable compression for price data
        try:
            await conn.execute(
                text(
                    "ALTER TABLE energy_price_points SET ("
                    "timescaledb.compress, "
                    "timescaledb.compress_segmentby = 'area_code, data_type, business_type', "
                    "timescaledb.compress_orderby = 'timestamp DESC'"
                    ");"
                )
            )
            self.logger.info("TimescaleDB compression enabled for energy_price_points")
        except SQLAlchemyError:
            # Compression might already be enabled
            self.logger.debug("Price data compression setup skipped")

        # Add compression policy to automatically compress chunks older than 7 days
        try:
            await conn.execute(
                text(
                    "SELECT add_compression_policy('energy_data_points', INTERVAL '7 days');"
                )
            )
            self.logger.info(
                "TimescaleDB compression policy configured for energy_data_points"
            )
        except SQLAlchemyError:
            # Policy might already exist
            self.logger.debug("Energy data compression policy setup skipped")

        # Add compression policy for price data
        try:
            await conn.execute(
                text(
                    "SELECT add_compression_policy('energy_price_points', INTERVAL '7 days');"
                )
            )
            self.logger.info(
                "TimescaleDB compression policy configured for energy_price_points"
            )
        except SQLAlchemyError:
            # Policy might already exist
            self.logger.debug("Price data compression policy setup skipped")

    def _validate_api_configuration(self) -> None:
        """Validate ENTSO-E API configuration."""
        try:
            settings = self.container.config()
            api_token = settings.entsoe_client.api_token.get_secret_value()
            if not api_token or len(api_token) < MIN_API_TOKEN_LENGTH:
                self.logger.error("ENTSO-E API token is missing or invalid")
                self.logger.error(
                    "Please set ENTSOE_CLIENT__API_TOKEN environment variable"
                )
                sys.exit(1)
            self.logger.info("ENTSO-E API configuration validated")
        except Exception:
            self.logger.exception("Failed to validate ENTSO-E API configuration")
            sys.exit(1)

    async def _start_scheduler_service(self) -> Any:
        """Start the scheduler service."""
        try:
            scheduler_service = self.container.scheduler_service()
            start_result = await scheduler_service.start()

            if not start_result.success:
                self.logger.error("Failed to start scheduler: %s", start_result.message)
                self.logger.error(
                    "Check your configuration and ensure all required services are available"
                )
                sys.exit(1)
            else:
                self.logger.info("Scheduler service started successfully")
                return scheduler_service
        except Exception:
            self.logger.exception("Failed to initialize scheduler service")
            sys.exit(1)

    async def _start_api_server(self) -> uvicorn.Server:
        """Start the FastAPI server for REST API access."""
        try:
            self.logger.info("Starting FastAPI API server...")

            # Create FastAPI application using existing factory
            app = create_app()

            # Get configuration from container
            settings = self.container.config()

            # Configure uvicorn server
            config = uvicorn.Config(
                app=app,
                host=settings.http.host,
                port=settings.http.port,
                log_level=settings.logging.level.lower(),
                access_log=settings.http.access_log,
                use_colors=False,  # Better for container logs
            )

            server = uvicorn.Server(config)

            # Start server in background task
            self.api_task = asyncio.create_task(server.serve())

            self.logger.info(
                "FastAPI server started on %s:%s",
                settings.http.host,
                settings.http.port,
            )
        except Exception:
            self.logger.exception("Failed to start API server")
            raise
        else:
            return server

    async def _shutdown_services(
        self, scheduler_service: Any, api_server: uvicorn.Server
    ) -> None:
        """Gracefully shutdown both scheduler and API services."""
        self.logger.info("Initiating graceful shutdown of all services...")

        # Stop accepting new API requests
        self.logger.info("Stopping API server...")
        api_server.should_exit = True

        # Stop scheduler service
        self.logger.info("Stopping scheduler service...")
        stop_result = await scheduler_service.stop()
        if stop_result.success:
            self.logger.info("Scheduler service stopped successfully")
        else:
            self.logger.error("Error stopping scheduler: %s", stop_result.message)

        # Wait for API task to complete
        if hasattr(self, "api_task") and not self.api_task.done():
            self.logger.info("Waiting for API server to finish...")
            try:
                await asyncio.wait_for(self.api_task, timeout=30.0)
                self.logger.info("API server stopped successfully")
            except TimeoutError:
                self.logger.warning(
                    "API server shutdown timed out, forcing cancellation"
                )
                self.api_task.cancel()
                with contextlib.suppress(asyncio.CancelledError):
                    await self.api_task

        self.logger.info("All services stopped successfully")

    async def _perform_startup_tasks(self, settings: Any) -> None:
        """Perform startup backfill and data verification tasks."""
        # Perform startup backfill analysis and fill gaps (if enabled)
        if settings.backfill.startup_backfill_enabled:
            await self._perform_startup_backfill()
            self.logger.info("Startup backfill analysis completed")
        else:
            self.logger.info("Startup backfill disabled in configuration")

        # Verify data collection and show summary (if enabled)
        if settings.backfill.startup_data_verification_enabled:
            await self._verify_data_collection()
        else:
            self.logger.info("Startup data verification disabled in configuration")

    async def run(self) -> None:
        """Run the unified service with both scheduler and API server."""
        self.setup_signal_handlers()

        scheduler_service = None
        api_server = None

        try:
            self.logger.info("=" * 60)
            self.logger.info("🔌 Energy Data Service Starting")
            self.logger.info("=" * 60)

            # Initialize database and create tables
            await self._initialize_database()

            # Validate ENTSO-E API configuration
            self._validate_api_configuration()
            settings = self.container.config()

            # Start both services
            scheduler_service = await self._start_scheduler_service()
            api_server = await self._start_api_server()

            # Perform startup tasks
            await self._perform_startup_tasks(settings)

            # Both services ready
            self.logger.info("=" * 60)
            self.logger.info("✅ Energy Data Service Ready")
            self.logger.info("📊 Scheduler: Collecting data every 30 minutes")
            self.logger.info(
                "🌐 API: Available at http://%s:%s/docs",
                settings.http.host,
                settings.http.port,
            )
            self.logger.info("=" * 60)

            # Wait for shutdown signal
            self.logger.info("Press Ctrl+C to stop the service")
            await self.shutdown_event.wait()

            # Graceful shutdown of both services
            if scheduler_service and api_server:
                await self._shutdown_services(scheduler_service, api_server)

        except KeyboardInterrupt:
            self.logger.info("Received keyboard interrupt")
            if scheduler_service and api_server:
                await self._shutdown_services(scheduler_service, api_server)
        except Exception:
            self.logger.exception("Application error")
            if scheduler_service and api_server:
                try:
                    await self._shutdown_services(scheduler_service, api_server)
                except Exception:
                    self.logger.exception("Error during emergency shutdown")
            sys.exit(1)
        finally:
            self.logger.info("Shutdown complete")


async def main() -> None:
    """Main entry point."""
    runner = SimpleSchedulerRunner()
    await runner.run()


if __name__ == "__main__":
    asyncio.run(main())
